---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 1 with R (STAT 301-1)
author: "Valerie Chu"
date: today

format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true

execute:
  warning: false
  
from: markdown+emoji 

---

::: {.callout-tip icon=false}

## Github Repo

[https://github.com/stat301-1-2023-fall/final-project-1-valerieyichu.git](https://github.com/stat301-1-2023-fall/final-project-1-valerieyichu.git)

:::

## Set Up

- Load Packages
```{r, echo = FALSE}
library(tidyverse)
```

- Load Data

```{r}
books <- read_delim("data/books.csv")
books_tags <- read_csv("data/book_tags.csv")
ratings <- read_csv("data/ratings.csv")
tags <- read_csv("data/tags.csv")
tbr <- read_csv("data/to_read.csv")
```
::: {.callout-tip icon=false}

## My Data Source

**[https://github.com/zygmuntz/goodbooks-10k](https://github.com/zygmuntz/goodbooks-10k)**

:::

## A Quick Refresher On My Data

**What is Goodreads?**

Officially, this is how Goodreads describes itself: “Goodreads is the world’s largest site for readers and book recommendations. Our mission is to help readers discover books they love and get more out of reading. Goodreads launched in January 2007.”

**What is my data about?**

This is an extremely comprehensive dataset for book data. 

It comes in five separate csv files: "books", “books_tags”, “ratings”, “tags”, and “tbr”

(For context, users on Goodreads can *tag* books and add them to their shelves. And "tbr" stands for "to be read".) 

In this document, when I say "Goodreads data", I am referring to the five datasets generally. When I join the datasets and give them a more specific name, that specific name is what I will use. 


## My Objectives

There are several questions I'm interested in examining in Goodreads data. 

- What are the most highly rated books? 

- What is the relationship between a book's average rating and the year it was published? 

- Do readers who leave the most ratings leave the higher ratings on average or lower ratings on average? 

- How do users tag the most highly rated books? Is there a trend?
grouping the different ratings (if treating rating as a category) --> stacked barplot, etc. 

## Joining Data

The README in my data source has some notes userful for joining the five Goodreads datasets. I've copied and pasted the notes I found most interesting and relevant below. I'll re-explain the key points when I join my data in the following sections, but I just wanted to put the most interesting notes about the Goodreads data together in the same section:

**Ratings**

- Ratings go from one to five. Both book IDs and user IDs are contiguous. For books, they are 1-10000, for users, 1-53424.

- to_read.csv provides IDs of the books marked "to read" by each user, as user_id,book_id pairs, sorted by time. There are close to a million pairs.

- books.csv has metadata for each book (goodreads IDs, authors, title, average rating, etc.). The metadata have been extracted from goodreads XML files, available in books_xml.

**Tags**

- book_tags.csv contains tags/shelves/genres assigned by users to books. Tags in this file are represented by their IDs. They are sorted by goodreads_book_id ascending and count descending.

- Each tag/shelf is given an ID. tags.csv translates tag IDs to names.

**Goodreads IDs**

- Each book may have many editions. goodreads_book_id and best_book_id generally point to the most popular edition of a given book, while goodreads work_id refers to the book in the abstract sense.

- Note that book_id in ratings.csv and to_read.csv maps to work_id, not to goodreads_book_id, meaning that ratings for different editions are aggregated.


## Step 1: Joining tags and books_tags

**Why am I joining tags and books_tags?**

I want to join tags and books_tags. This is because each `goodreads_book_id` within the books_tags dataset has a `tag_id`. And each `tag_name` within the tags dataset has a `tag_id`. In other words, when I join the books_tags dataset together with the tags dataset, I can figure out what each `goodreads_book_id` was tagged with (`tag_name`). I would still have to match each `goodreads_book_id` with its title, but that's another join that I'll do later on. 

And just to recap, the reason why I want to figure out what each `goodreads_book_id` was tagged with is so that I can start to answer my research question about if there's a trend in how users tag the most highly rated books. 


**Now let's actually join tags and books_tags**

One of the things we learned from the README:

tags.csv and books_tags.csv both have the variable `tag_id` in common. And `tag_id` has only unique values for both datasets. `tag_id` also corresponds for both datasets. So let's test whether `tag_id` is a feasible foreign / primary key:

First, an exploration of implicit missing variables in the tags and books_tags datasets:
```{r, echo = FALSE}
anti_join(tags, books_tags, by = c("tag_id" = "tag_id")) 
```
```{r, echo = FALSE}
anti_join(books_tags, tags, by = c("tag_id" = "tag_id")) 
```
Using anti_joins shows us that there are no `tag_id` rows in `books_tags` that don't have a matching `tag_id` row in `tags`. In other words, every `tag_id` is unique and each `tag_id` row in both datasets correspond directly to each other. 

Also, we can check double this. If we take the `count` of `tag_id` in both datasets, the number of rows is exactly the same: 34,252 rows:
```{r, echo = FALSE}
books_tags |> count(tag_id)
tags |> count(tag_id)
```

So **`tag_id` is the primary key for both datasets and also a foreign key that can be used to join the two datasets.**

Now, let's actually join the two datasets: 

To do this, I'm going to use a full_join (it won't really matter whether I use a full_join, left_join, right_join, or inner_join in this case because `tag_id` in both datasets perfectly correspond). 

An excerpt of **the new books_and_tags dataset:**

```{r, echo = FALSE}
# Join tags and books_tags

books_and_tags <- full_join(tags, books_tags, join_by(tag_id == tag_id))

books_and_tags

```

Step 1 is done. Now, before I get to the EDA of our new dataset, books_and_tags, I want to join another two datasets:  


## Step 2: Joining books and ratings

**Why am I joining books and ratings?**

Joining books and ratings will allow me to start answering on three of my research questions: "What are the most highly rated books?", "What is the relationship between a book’s average rating and the year it was published?" and "Do readers who leave the most ratings leave the higher ratings on average or lower ratings on average?" 


**Now let's actually join books and ratings**

But first, some prefaces:

The README says:

- Both `book_id` and `user_id` are contiguous in the ratings.csv file.

- It also contains an important note: `book_id` in the ratings dataset and tbr dataset map to `work_id` in the books dataset (not `goodreads_book_id` as we would intuitively assume). 

This [article linked on the About section of the github where I found the Goodreads dataset](http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/) says:

- "Each book may have many editions. `goodreads_book_id` and `best_book_id` generally point to the most popular edition of a given book, while goodreads `work_id` refers to the book in the abstract sense." 

- It also again states that "Note that book_id in ratings.csv and to_read.csv (I, Valerie, called this the  "tbr" dataset) maps to `work_id`, not to goodreads_book_id. **It means that ratings for different editions are aggregated.**

I read through the README of the [dataset](https://github.com/zygmuntz/goodbooks-10k), the [article linked on the dataset](http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/), and [the Kaggle which has descriptions for the previous, unupdated version of the github dataset I'm using](https://www.kaggle.com/datasets/zygmunt/goodbooks-10k#books.csv). 

From these three links, I've arrived at several conclusions:

- Mapping `book_id` in the ratings dataset to `work_id` in the books dataset is only necessary if I care about different editions being aggregated. I don't. I'm more interested in the most popular edition of a given book. That's what `goodreads_book_id` tracks, according to the article. 

- And `books_id` in the books dataset *should be* equivalent to `books_id` in the ratings dataset. So I'm going to see if I can join the books and ratings dataset using `books_id` as the key: 


Let's start by seeing whether there is implicit missingness using antijoins:

```{r, echo = FALSE}
anti_join(books, ratings, by = c("book_id" = "book_id"))
```

The code above returns all rows for `book_id` within books that don't have a matching `book_id` within rating and vice versa. There are no rows that don't match. 

So **book_id is the primary key for both datasets and also a foreign key that can be used to join the two datasets.**

Just a quick side note because I find this very interesting: If I were to have instead mapped `work_id` in books with `book_id` in ratings (which I didn't because I'm interested in the most popular edition of a given book, *not* different editions being aggregated). But just hypothetically speaking. Then I would have first needed to condense the ratings into summarized outputs then join that with books using a left join (because there are 9,824 rows where `work_id` doesn't match to `book_id` and 5,846,293 rows where `book_id` doesn't match to `work_id` (there are simply many more work_id rows than book_id rows because, again, multiple book editions. See my R file for more details). But since I'm not doing that because I'm not interested in different editions being aggregated, all I need to do is use `book_id` to join the datasets books and ratings.  

An excerpt of **the new books_and_ratings dataset:**

```{r, echo = FALSE}
books_and_ratings <- full_join(books, ratings, join_by(book_id == book_id)) |> 
  group_by(title) |> 
  select(title, book_id, goodreads_book_id, work_id, user_id, rating)

books_and_ratings

```

Now, for the fun part. An EDA with our two new datasets, books_and_tags and books_and_ratings. And maybe some of the original datasets too. 


## Question 1: What are the most highly rated books?








# SCRATCH WORK IGNORE BELOW

**Data should be merged, cleaned, and variables should be explored. Univariate and bivariate analysis completed for several variables.** 
If your tibble is particularly big, put an appendix. Or if you have miscellaneous information that's not that important but that you really want to show, put it in an appendix. 

Either use ggsave or echo false. 


## Load Data
```{r}
library(tidyverse)
```



After you do all this data cleaning, save it as an RDS to preserve variable types!! That way you can read it in again with each script. 

Here's a demo of what it could look like:
```{r}
#| echo: false

tibble <- tibble(name = c("var1") , value = c(1))

tibble |> 
  knitr::kable()

```

Don't show a skim or a summary output. INSTEAD, make it look nice. 

- Ex. Calculate summary stats with summarize() to output as a nice table. Mean, sd, correlation, etc. 

- Ex. Make a scatterplot, a boxplot, etc. 


## Basic objectives
Objective 1
Students are expected to setup their own qmd file to render to an html for this project. The document should be appropriately formatted (see previous memo and other assignments). Should have a title, author, date, and appropriate headers, and sub-headers.

## Objective 2
Students are expected to demonstrate that significant progress has been made on their final project since the submission of progress memo 1. Students should have their data cleaned and the EDA should be started.

Demonstrating significant progress means students should have some univariate and bivariate analyses complete for several of their variables.  They should share a few graphics and/or tables with a description of what they have found thus far to demonstrate they progress. Students should should clearly state what they are exploring and why in these demonstrations. That is, they should share the guiding curiosity or research question that accompanies the particular graphics and/or tables they choose to share. 

## What else should be in the memo
Students should summarize their progress, where they are at, and what their next steps will be. Self assessment of progress would also be appropriate. When thinking or describing next steps students should share any guiding curiosities or research questions they plan to explore.

## Misc Notes/Comments
Final GitHub repo link should be at the top in a callout block --- similar to all other assignments
There should be no code visible or accessible in memo. 
There should be no raw R output like tibbles/data frames. Use html tables.
In rare cases where the the project is extremely heavy on advanced data collection, progress will look quite different. Projects like this will be focused on showing progress on getting the data together and collected. If your project falls in this category, then discuss this with your instructor --- very few, if any projects, fall into this rare case. 

