---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 1 with R (STAT 301-1)
author: "Valerie Chu"
date: today

format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true

execute:
  warning: false
  
from: markdown+emoji 

---

## Load Packages
```{r}
library(tidyverse)
```

[https://github.com/stat301-1-2023-fall/final-project-1-valerieyichu.git](https://github.com/stat301-1-2023-fall/final-project-1-valerieyichu.git)

## Load Data

```{r}
books <- read_delim("data/books.csv")
books_tags <- read_csv("data/book_tags.csv")
ratings <- read_csv("data/ratings.csv")
tags <- read_csv("data/tags.csv")
tbr <- read_csv("data/to_read.csv")
```
::: {.callout-tip icon=false}

## My Data Source

**[https://github.com/zygmuntz/goodbooks-10k](https://github.com/zygmuntz/goodbooks-10k)**

:::

## A Quick Refresher On My Data

**What is Goodreads?**

Officially, this is how Goodreads describes itself: “Goodreads is the world’s largest site for readers and book recommendations. Our mission is to help readers discover books they love and get more out of reading. Goodreads launched in January 2007.”

**What is my data about?**

This is an extremely comprehensive dataset for book data. 

It comes in five separate csv files: "books", “books_tags”, “ratings”, “tags”, and “tbr”

(For context, users on Goodreads can *tag* books and add them to their shelves. And "tbr" stands for "to be read".) 

In this document, when I say "Goodreads data", I am referring to the five datasets generally. When I join the datasets and give them a more specific name, that specific name is what I will use. 


## My Objectives

There are several questions I'm interested in examining in Goodreads data. 

- What are the most highly rated books? 

- What is the relationship between a book's average rating and the year it was published? 

- How do users tag the most highly rated books? Is there a trend?
grouping the different ratings (if treating rating as a category) --> stacked barplot, etc. 

## Joining Data

The README in my data source has some notes userful for joining the five Goodreads datasets. I've copied and pasted the notes I found most interesting and relevant below:

**Ratings**

- Ratings go from one to five. Both book IDs and user IDs are contiguous. For books, they are 1-10000, for users, 1-53424.

- to_read.csv provides IDs of the books marked "to read" by each user, as user_id,book_id pairs, sorted by time. There are close to a million pairs.

- books.csv has metadata for each book (goodreads IDs, authors, title, average rating, etc.). The metadata have been extracted from goodreads XML files, available in books_xml.

**Tags**

- book_tags.csv contains tags/shelves/genres assigned by users to books. Tags in this file are represented by their IDs. They are sorted by goodreads_book_id ascending and count descending.

- Each tag/shelf is given an ID. tags.csv translates tag IDs to names.

**Goodreads IDs**

- Each book may have many editions. goodreads_book_id and best_book_id generally point to the most popular edition of a given book, while goodreads work_id refers to the book in the abstract sense.

- Note that book_id in ratings.csv and to_read.csv maps to work_id, not to goodreads_book_id, meaning that ratings for different editions are aggregated.


## Step 1: Joining tags with book_tags


One of the things we learned from the README:

tags.csv and books_tags.csv both have the variable `tag_id` in common. And `tag_id` has only unique values for both datasets. `tag_id` also corresponds for both datasets. So let's test whether `tag_id` is a feasible foreign / primary key:

First, a data exploration of implicit missing variables:
```{r}
anti_join(tags, books_tags, by = c("tag_id" = "tag_id")) 
```
```{r}
anti_join(books_tags, tags, by = c("tag_id" = "tag_id")) 
```
Using anti_joins shows us that there are no `tag_id` rows in `books_tags` that don't have a matching `tag_id` row in `tags`. In other words, every `tag_id` is unique and each `tag_id` row in both datasets correspond directly to each other. 

Also, if we take the `count` of `tag_id` in both datasets, the number of rows is exactly the same: 34,252 rows
```{r}
books_tags |> count(tag_id)
tags |> count(tag_id)
```



So:

**`tag_id` is the primary key for both datasets and also a foreign key that can be used to join the two datasets.**


Now, let's actually join the two datasets: 

To do this, I'm going to use a full_join (it won't really matter whether I use a full_join, left_join, right_join, or inner_join in this case because `tag_id` in both datasets perfectly correspond): 

```{r}
# Join tags and books_tags

books_and_tags <- full_join(tags, books_tags, join_by(tag_id == tag_id))

books_and_tags

```

Now, for the fun part. An EDA of our new dataset, books_and_tags. 


## Step 2: Joining books and ratings

The README says that both `book_id` and `user_id` are contiguous in the ratings.csv file.

It also contains a very important note: `book_id` in the ratings dataset and tbr dataset map to `work_id` in the books dataset (not `goodreads_book_id` as we would intuitively assume). 

Right now, I just want to focus on my first question: "What are the most highly rated books?" 

And to do so, I need to join the ratings dataset with the books dataset. 

Let's start by seeing whether there is implicit missingness using antijoins:

```{r}
anti_join(books, ratings, by = c("book_id" = "book_id"))
```

The code above returns all rows for `work_id` within books that don't have a matching `book_id` within rating and vice versa.


Interestingly, the antijoins reveal that there are 9,824 rows missing. So that means that `book_id` from rating and `work_id` from books have the same number of implicitly missing values. I wonder why that is. 

Let's use full_join then filter for NA values. 

 ful join includes books that don't have a rating and ratings that aren't paired with books.\
maybe - summarize ratings by books then join them
left join books

0. misisngness .merge issues. summary statistics. weird things happening/outliers to deal with/unusual things.
1. make sure factors are factors, characters are characters, etc. 
3. Research questions


1. condense ratings into summarized outputs. 
2. mess around with the order of the left join. 


```{r}
books_and_ratings <- full_join(books, ratings, join_by(book_id == book_id)) |> 
  select(title, work_id, book_id) 

books_and_ratings 
```





# SCRATCH WORK IGNORE BELOW

**Data should be merged, cleaned, and variables should be explored. Univariate and bivariate analysis completed for several variables.** 
If your tibble is particularly big, put an appendix. Or if you have miscellaneous information that's not that important but that you really want to show, put it in an appendix. 

Either use ggsave or echo false. 


## Load Data
```{r}
library(tidyverse)
```



After you do all this data cleaning, save it as an RDS to preserve variable types!! That way you can read it in again with each script. 

Here's a demo of what it could look like:
```{r}
#| echo: false

tibble <- tibble(name = c("var1") , value = c(1))

tibble |> 
  knitr::kable()

```

Don't show a skim or a summary output. INSTEAD, make it look nice. 

- Ex. Calculate summary stats with summarize() to output as a nice table. Mean, sd, correlation, etc. 

- Ex. Make a scatterplot, a boxplot, etc. 


## Basic objectives
Objective 1
Students are expected to setup their own qmd file to render to an html for this project. The document should be appropriately formatted (see previous memo and other assignments). Should have a title, author, date, and appropriate headers, and sub-headers.

## Objective 2
Students are expected to demonstrate that significant progress has been made on their final project since the submission of progress memo 1. Students should have their data cleaned and the EDA should be started.

Demonstrating significant progress means students should have some univariate and bivariate analyses complete for several of their variables.  They should share a few graphics and/or tables with a description of what they have found thus far to demonstrate they progress. Students should should clearly state what they are exploring and why in these demonstrations. That is, they should share the guiding curiosity or research question that accompanies the particular graphics and/or tables they choose to share. 

## What else should be in the memo
Students should summarize their progress, where they are at, and what their next steps will be. Self assessment of progress would also be appropriate. When thinking or describing next steps students should share any guiding curiosities or research questions they plan to explore.

## Misc Notes/Comments
Final GitHub repo link should be at the top in a callout block --- similar to all other assignments
There should be no code visible or accessible in memo. 
There should be no raw R output like tibbles/data frames. Use html tables.
In rare cases where the the project is extremely heavy on advanced data collection, progress will look quite different. Projects like this will be focused on showing progress on getting the data together and collected. If your project falls in this category, then discuss this with your instructor --- very few, if any projects, fall into this rare case. 

